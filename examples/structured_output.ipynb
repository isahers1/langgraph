{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools and structured output\n",
    "\n",
    "The ability of LLMs to respond in structured format is helpful for a variety of use cases such as when we want to call functions with rigid parameters or update databases with defined schemas.\n",
    "\n",
    "In Langgraph there are a few different ways we can utilize tools and structured outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using ToolNode\n",
    "\n",
    "In some cases, you might want to directly execute a tool node after calling your LLM. Perhaps you have made a weather chatbot such that whenever a question is asked, you want to immediately call a tool to pull the latest weather data. \n",
    "\n",
    "In this case, we can take  advantage of the built in [`ToolNode`](https://langchain-ai.github.io/langgraph/reference/prebuilt/#toolnode) functionality, and let Langgraph do most of the work for us. By using `ToolNode` we can create a node that will automatically call the tool that was referenced in the last message in the chain. In our example we are creating a `ToolNode` with just a single tool, but you can pass multiple tools and the node will automatically select which one to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "tools = [TavilySearchResults(max_results=1)]\n",
    "tool_node = ToolNode(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just defining our tool and creating a corresponding [`ToolNode`](https://langchain-ai.github.io/langgraph/reference/prebuilt/#toolnode) isn't enough - we need to make our LLM aware that it has the option to call such tools. We can do this by using the `bind_tools` method. In our example we actually want to force our LLM to call a specific tool, so we can use the `tool_choice` param to ensure this always happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, streaming=True)\n",
    "model_with_tools = model.bind_tools(tools,tool_choice='tavily_search_results_json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the State class we will use for our graph - in our example it will just contain one member variable called `messages` which is a list of all the messages generated by our chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Literal\n",
    "\n",
    "def add_messages(left: list, right: list):\n",
    "    \"\"\"Add-don't-overwrite.\"\"\"\n",
    "    return left + right\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    # The `add_messages` function within the annotation defines\n",
    "    # *how* updates should be merged into the state.\n",
    "    messages: Annotated[list, add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define the function for our model node and the layout of our graph. Since we are forcing it to call the tool - we don't need to define a routing function, we can just add an edge between our model node and our tool node. After calling our tool, we will route to another LLM node which will return a plain text response to our user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END, START\n",
    "\n",
    "def call_model_with_tools(state: AgentState) -> AgentState:\n",
    "    return {\"messages\":[model_with_tools.invoke(state['messages'])]}\n",
    "\n",
    "def call_model(state: AgentState) -> AgentState:\n",
    "    return {\"messages\":[model.invoke(state['messages'])]}\n",
    "\n",
    "graph = StateGraph(AgentState)\n",
    "graph.add_node(\"agent\", call_model_with_tools)\n",
    "graph.add_node(\"respond_to_search\", call_model)\n",
    "graph.add_node(\"search\", tool_node)\n",
    "graph.add_edge(START, \"agent\")\n",
    "graph.add_edge(\"agent\",\"search\")\n",
    "graph.add_edge(\"search\",\"respond_to_search\")\n",
    "graph.add_edge(\"respond_to_search\",END)\n",
    "app = graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's invoke our graph and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the weather in Barcelona?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search_results_json (call_IrjsLY2IuxvVs9K6PHxBFkjm)\n",
      " Call ID: call_IrjsLY2IuxvVs9K6PHxBFkjm\n",
      "  Args:\n",
      "    query: weather in Barcelona\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"url\": \"https://www.weatherapi.com/\", \"content\": \"{'location': {'name': 'Barcelona', 'region': 'Catalonia', 'country': 'Spain', 'lat': 41.38, 'lon': 2.18, 'tz_id': 'Europe/Madrid', 'localtime_epoch': 1718211272, 'localtime': '2024-06-12 18:54'}, 'current': {'last_updated_epoch': 1718210700, 'last_updated': '2024-06-12 18:45', 'temp_c': 21.1, 'temp_f': 70.0, 'is_day': 1, 'condition': {'text': 'Partly cloudy', 'icon': '//cdn.weatherapi.com/weather/64x64/day/116.png', 'code': 1003}, 'wind_mph': 11.9, 'wind_kph': 19.1, 'wind_degree': 220, 'wind_dir': 'SW', 'pressure_mb': 1016.0, 'pressure_in': 30.0, 'precip_mm': 0.15, 'precip_in': 0.01, 'humidity': 60, 'cloud': 50, 'feelslike_c': 21.1, 'feelslike_f': 70.0, 'windchill_c': 19.8, 'windchill_f': 67.6, 'heatindex_c': 19.8, 'heatindex_f': 67.6, 'dewpoint_c': 13.6, 'dewpoint_f': 56.5, 'vis_km': 10.0, 'vis_miles': 6.0, 'uv': 4.0, 'gust_mph': 15.7, 'gust_kph': 25.2}}\"}]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The current weather in Barcelona is partly cloudy with a temperature of 21.1Â°C (70.0Â°F). The wind is coming from the southwest at 19.1 km/h (11.9 mph) and the humidity is at 60%.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "ans = app.invoke({'messages':[HumanMessage(content=\"What is the weather in Barcelona?\")]})\n",
    "for message in ans['messages']:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we used three nodes, but you could simplify your graph to just use two nodes and a routing function that determines whether to proceed from the LLM node to the tool node. Later in this page we will show examples of how to use routing functions with tool calls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize the tool node\n",
    "\n",
    "Expanding on the above example, we can customize our tool nodes in many ways. One way we might customize our tool node is to update the state within the tool node, or pass in additional parameters to our tool node that come from sources other than the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine how we could customize the parameters we pass to our tool node. In our case we will customize the `max_results` param we pass to Tavily using user input, but we could also augment the LLM output with values from our state, values from other tool nodes, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from langgraph.prebuilt import ToolExecutor\n",
    "\n",
    "@tool\n",
    "def search(query: str, max_results: int):\n",
    "    '''Look things up online'''\n",
    "    return [str(TavilySearchResults(max_results=max_results).invoke(query)).replace('\\'','\\\"')]\n",
    "\n",
    "\n",
    "tools = [search]\n",
    "tool_executor = ToolExecutor(tools)\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, streaming=True)\n",
    "model_with_tools = model.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using the built in [`ToolNode`](https://langchain-ai.github.io/langgraph/reference/prebuilt/#toolnode) functionality, we can now define a custom function for our tool node that will ask the user for how many search results they want returned, and then call our `search` tool with the right input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolInvocation\n",
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "def execute_search_with_user_input(state: AgentState):\n",
    "    max_results = input(prompt=\"How many search results should we return? Enter a number 1-10\")\n",
    "\n",
    "    if max_results.isdigit() and int(max_results) > 0 and int(max_results) <= 10:\n",
    "        last_message = state['messages'][-1]\n",
    "        tool_call = last_message.tool_calls[0]\n",
    "        action = ToolInvocation(\n",
    "            tool=tool_call[\"name\"],\n",
    "            tool_input={**tool_call[\"args\"],**{'max_results':int(max_results)}},\n",
    "        )\n",
    "        response = tool_executor.invoke(action)\n",
    "        return {'messages':[ToolMessage(content=str(response),tool_call_id=tool_call['id'])]}\n",
    "    else:\n",
    "        raise ValueError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define our graph very similarly to the one above, and run it again to examine the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(AgentState)\n",
    "graph.add_node(\"agent\", call_model_with_tools)\n",
    "graph.add_node(\"respond_to_search\", call_model)\n",
    "graph.add_node(\"search\", execute_search_with_user_input)\n",
    "graph.add_edge(START, \"agent\")\n",
    "graph.add_edge(\"agent\",\"search\")\n",
    "graph.add_edge(\"search\",\"respond_to_search\")\n",
    "graph.add_edge(\"respond_to_search\",END)\n",
    "app = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the weather in Barcelona?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  search (call_H82UfLj030KPvA1PoAxa8G2o)\n",
      " Call ID: call_H82UfLj030KPvA1PoAxa8G2o\n",
      "  Args:\n",
      "    query: weather in Barcelona\n",
      "    max_results: 1\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "['[{\"url\": \"https://www.weatherapi.com/\", \"content\": \"{\"location\": {\"name\": \"Barcelona\", \"region\": \"Catalonia\", \"country\": \"Spain\", \"lat\": 41.38, \"lon\": 2.18, \"tz_id\": \"Europe/Madrid\", \"localtime_epoch\": 1718212317, \"localtime\": \"2024-06-12 19:11\"}, \"current\": {\"last_updated_epoch\": 1718211600, \"last_updated\": \"2024-06-12 19:00\", \"temp_c\": 19.4, \"temp_f\": 66.9, \"is_day\": 1, \"condition\": {\"text\": \"Partly cloudy\", \"icon\": \"//cdn.weatherapi.com/weather/64x64/day/116.png\", \"code\": 1003}, \"wind_mph\": 6.9, \"wind_kph\": 11.2, \"wind_degree\": 110, \"wind_dir\": \"ESE\", \"pressure_mb\": 1017.0, \"pressure_in\": 30.03, \"precip_mm\": 0.52, \"precip_in\": 0.02, \"humidity\": 64, \"cloud\": 75, \"feelslike_c\": 19.4, \"feelslike_f\": 66.9, \"windchill_c\": 19.3, \"windchill_f\": 66.8, \"heatindex_c\": 19.3, \"heatindex_f\": 66.8, \"dewpoint_c\": 13.6, \"dewpoint_f\": 56.4, \"vis_km\": 10.0, \"vis_miles\": 6.0, \"uv\": 4.0, \"gust_mph\": 14.6, \"gust_kph\": 23.5}}\"}, {\"url\": \"https://www.weather25.com/europe/spain/catalonia/barcelona?page=month&month=June\", \"content\": \"Full weather forecast for Barcelona in June 2024. Check the temperatures, chance of rain and more in Barcelona during June. ... Jun 12. 4.5 mm. 21 ... 06. June. 07. July. 08. August. 09. September. 10. October. 11. November. 12. December. Barcelona annual weather. Month Temperatures Rainy Days Dry Days Snowy Days\"}, {\"url\": \"https://world-weather.info/forecast/spain/barcelona/june-2024/\", \"content\": \"Extended weather forecast in Barcelona. Hourly Week 10 days 14 days 30 days Year. Detailed â¡ Barcelona Weather Forecast for June 2024 - day/night ğ¡ï¸ temperatures, precipitations - World-Weather.info.\"}, {\"url\": \"https://www.barcelona.com/weather_in_barcelona\", \"content\": \"next time ;-)\\\\na bit cold this month of May\\\\n- Tania3000 (11 May 2023 - 08:38)\\\\nthe temperature in Barcelona is effectively around 18Â° celsius in may..\\\\na bit cold ;-( but when the sun comes out it is immediately better\\\\ncold and rain\\\\n- meagain (10 May 2023 - 14:02)\\\\ncold and rainy, that\"s the weather forecast in Barcelona for my next trip.\\\\n It\"s absolutely pouring with rain ...\\\\nWeather April\\\\n- Morefews.dk (14 Apr 2017 - 09:36)\\\\nApril has nice weather\\\\nraining\\\\n- grrr (1 Apr 2017 - 15:04)\\\\nnot funny.. my first time in Barcelona and f... raining\\\\na bit cold\\\\n- brrr (15 Feb 2017 - 08:38)\\\\na bit cold today in Barcelona.. when I think that the rest of Spain is over 30Â°Celsius...\\\\nenjoy these few rains and this coolness before the summer ;-)\\\\nForest Fires: CataluÃ±a,\\\\nCastellon &Teruel\\\\n- LOLITA (27 Mar 2023 - 08:06)\\\\nBeware and stay safe everyone..\\\\n@flo\\\\n- Barcelona.com (11 Mar 2023 - 14:22)\\\\n I\"m desperate ;-(\\\\nnot yet the right week\\\\n- wolf (1 Jun 2023 - 16:04)\\\\nweather in barcelona so gray and rainy...\\\\nbad luck :-((\\\\nand it\"s raining this weekend ;-(\\\\n- tinabest (25 May 2023 - 09:28)\\\\n best,\\\\nWeather\\\\n- Alen (16 Feb 2022 - 17:34)\\\\nI would say so, yes\\\\nrain\\\\n- jeanne (12 Nov 2021 - 08:24)\\\\nHi\\\\non nextweek , the weather is good?\\\\n\"}]']\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The current weather in Barcelona is partly cloudy with a temperature of 19.4Â°C (66.9Â°F). The wind is coming from the east-southeast at 11.2 km/h (6.9 mph) with a humidity level of 64%.\n"
     ]
    }
   ],
   "source": [
    "ans = app.invoke({'messages':[HumanMessage(content=\"What is the weather in Barcelona?\")]})\n",
    "for message in ans['messages']:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the AI message passed a `max_results` value of 1, but by allowing the user to input a different value (in this case I entered 4), we can customize how the tool gets called."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way we could customize our tool node is to expand its functionality beyond just executing a tool call and have it update the graph state directly. To do this, let's add another field to our graph state called `urls` which will just keep track of all the URLs our search tool returns. This could be useful if we perhaps wanted to show the user where the LLM sourced its information from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_urls(old_urls, new_urls):\n",
    "    # Add URL lists together, skipping duplicates\n",
    "    for url in new_urls:\n",
    "        if url not in old_urls:\n",
    "            old_urls.append(url)\n",
    "    return old_urls\n",
    "\n",
    "class AgentStateComplex(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    urls: Annotated[list, add_urls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now redefine our tool node to update the state when called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def execute_search_and_update_state(state: AgentState):\n",
    "\n",
    "    last_message = state['messages'][-1]\n",
    "    tool_call = last_message.tool_calls[0]\n",
    "    action = ToolInvocation(\n",
    "        tool=tool_call[\"name\"],\n",
    "        tool_input={**tool_call[\"args\"],**{'max_results':3}},\n",
    "    )\n",
    "    response = tool_executor.invoke(action)\n",
    "    urls = [search_response['url'] for search_response in json.loads(response[0].replace(\"\\\"{\",\"{\").replace(\"}\\\"\",\"}\"))]\n",
    "    return {'messages':[ToolMessage(content=str(response),tool_call_id=tool_call['id'])],\"urls\":urls}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our graph is identical in structure to the example above, the only difference being a substitution for the tool node function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(AgentStateComplex)\n",
    "graph.add_node(\"agent\", call_model_with_tools)\n",
    "graph.add_node(\"respond_to_search\", call_model)\n",
    "graph.add_node(\"search\", execute_search_and_update_state)\n",
    "graph.add_edge(START, \"agent\")\n",
    "graph.add_edge(\"agent\",\"search\")\n",
    "graph.add_edge(\"search\",\"respond_to_search\")\n",
    "graph.add_edge(\"respond_to_search\",END)\n",
    "app = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.weatherapi.com/',\n",
       " 'https://www.timeanddate.com/weather/spain/barcelona/ext',\n",
       " 'https://www.bbc.com/weather/3128760']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = app.invoke({'messages':[HumanMessage(content=\"What is the weather in Barcelona?\")]})\n",
    "ans['urls']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our tool node correctly updated the state, allowing us to store the URLs it used in creating it's response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are just two examples of how to customize your tool node, and there are more options available to developers such as streaming nested inputs within a tool node, nesting further tool calls within a single tool node, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize routing and output\n",
    "\n",
    "In the examples above, we have been adding \"strict\" edges, in the sense that the LLM always proceeds automatically to calling the tool, which executes a single function. But this doesn't always need to be the case.\n",
    "\n",
    "In fact, tools donât have to be single functions, they are just ways for an LLM to generate structured output that your application can use. How it uses it is up to you. Some tools may not even correspond with a single scoped âtoolâ - they can be used for routing or for responding in a structured format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine how we can use tools to have our graph return data in a structured format. We can achieve this by using Pydantic models. Let's continue with the weather chatbot example and define our structured output using the `BaseModel` and `Field` classes from Pydantic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# The docstring tells our model that formatting should be called as the final response to the user\n",
    "class WeatherResponse(BaseModel):\n",
    "    \"\"\"Final response to the user\"\"\"\n",
    "\n",
    "    temperature: float = Field(description=\"the temperature\")\n",
    "    wind_speed: float = Field(description=\"the wind speed\")\n",
    "    wind_direction: str = Field(description=\"the direction of the wind\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create an LLM that uses `with_strctured_output` to return the result we would like to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "tools = [TavilySearchResults()]\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, streaming=True)\n",
    "model_with_tools = model.bind_tools(tools,tool_choice='tavily_search_results_json')\n",
    "model_with_structured_output = model.with_structured_output(WeatherResponse)\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "def call_structured_model(state: AgentState) -> AgentState:\n",
    "    return {\"messages\":[model_with_structured_output.invoke(state['messages'])]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our graph definition is again almost identical to the ones above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(AgentState)\n",
    "graph.add_node(\"agent\", call_model_with_tools)\n",
    "graph.add_node(\"respond_to_search_structured\", call_structured_model)\n",
    "graph.add_node(\"search\", tool_node)\n",
    "graph.add_edge(\"agent\",\"search\")\n",
    "graph.add_edge(\"search\",\"respond_to_search_structured\")\n",
    "graph.add_edge(\"respond_to_search_structured\",END)\n",
    "graph.add_edge(START, \"agent\")\n",
    "app = graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens when we run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the weather in Barcelona?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search_results_json (call_wSfe8i00QKlX0olftBkLDsdA)\n",
      " Call ID: call_wSfe8i00QKlX0olftBkLDsdA\n",
      "  Args:\n",
      "    query: weather in Barcelona\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"url\": \"https://www.weatherapi.com/\", \"content\": \"{'location': {'name': 'Barcelona', 'region': 'Catalonia', 'country': 'Spain', 'lat': 41.38, 'lon': 2.18, 'tz_id': 'Europe/Madrid', 'localtime_epoch': 1718214896, 'localtime': '2024-06-12 19:54'}, 'current': {'last_updated_epoch': 1718214300, 'last_updated': '2024-06-12 19:45', 'temp_c': 21.3, 'temp_f': 70.3, 'is_day': 1, 'condition': {'text': 'Moderate or heavy rain with thunder', 'icon': '//cdn.weatherapi.com/weather/64x64/day/389.png', 'code': 1276}, 'wind_mph': 10.5, 'wind_kph': 16.9, 'wind_degree': 200, 'wind_dir': 'SSW', 'pressure_mb': 1017.0, 'pressure_in': 30.03, 'precip_mm': 0.79, 'precip_in': 0.03, 'humidity': 64, 'cloud': 75, 'feelslike_c': 21.3, 'feelslike_f': 70.3, 'windchill_c': 19.6, 'windchill_f': 67.2, 'heatindex_c': 19.6, 'heatindex_f': 67.2, 'dewpoint_c': 13.8, 'dewpoint_f': 56.9, 'vis_km': 10.0, 'vis_miles': 6.0, 'uv': 4.0, 'gust_mph': 13.1, 'gust_kph': 21.0}}\"}, {\"url\": \"https://www.weather25.com/europe/spain/catalonia/barcelona?page=month&month=June\", \"content\": \"Full weather forecast for Barcelona in June 2024. Check the temperatures, chance of rain and more in Barcelona during June. ... Jun 12. 4.5 mm. 21 ... 06. June. 07. July. 08. August. 09. September. 10. October. 11. November. 12. December. Barcelona annual weather. Month Temperatures Rainy Days Dry Days Snowy Days\"}, {\"url\": \"https://world-weather.info/forecast/spain/barcelona/june-2024/\", \"content\": \"Extended weather forecast in Barcelona. Hourly Week 10 days 14 days 30 days Year. Detailed \\u26a1 Barcelona Weather Forecast for June 2024 - day/night \\ud83c\\udf21\\ufe0f temperatures, precipitations - World-Weather.info.\"}, {\"url\": \"https://weatherspark.com/h/y/47213/2024/Historical-Weather-during-2024-in-Barcelona-Spain\", \"content\": \"Barcelona Temperature History 2024\\nHourly Temperature in 2024 in Barcelona\\nCompare Barcelona to another city:\\nCloud Cover in 2024 in Barcelona\\nObserved Weather in 2024 in Barcelona\\nHours of Daylight and Twilight in 2024 in Barcelona\\nSunrise & Sunset with Twilight and Daylight Saving Time in 2024 in Barcelona\\nSolar Elevation and Azimuth in 2024 in Barcelona\\nMoon Rise, Set & Phases in 2024 in Barcelona\\nHumidity Comfort Levels in 2024 in Barcelona\\nWind Speed in 2024 in Barcelona\\nHourly Wind Speed in 2024 in Barcelona\\nHourly Wind Direction in 2024 in Barcelona\\nAtmospheric Pressure in 2024 in Barcelona\\nData Sources\\n 57\\u00b0F\\nPrecipitation\\nNo Report\\nWind\\n4.6 mph\\nCloud Cover\\nMostly Clear\\n3,500 ft\\nRaw: LEBL 141830Z VRB04KT While having the tremendous advantages of temporal and spatial completeness, these reconstructions: (1) are based on computer models that may have model-based errors, (2) are coarsely sampled on a 50 km grid and are therefore unable to reconstruct the local variations of many microclimates, and (3) have particular difficulty with the weather in some coastal areas, especially small islands.\\n We further caution that our travel scores are only as good as the data that underpin them, that weather conditions at any given location and time are unpredictable and variable, and that the definition of the scores reflects a particular set of preferences that may not agree with those of any particular reader.\\n See all nearby weather stations\\nLatest Report \\u2014 7:30 PM\\nSun, Jan 14, 2024\\u00a0\\u00a0\\u00a0\\u00a034 min ago\\u00a0\\u00a0\\u00a0\\u00a0UTC 18:30\\nCall Sign LEBL\\nTemp.\\n\"}, {\"url\": \"https://www.accuweather.com/en/es/barcelona/307297/june-weather/307297\", \"content\": \"Get the monthly weather forecast for Barcelona, Catalonia, Spain, including daily high/low, historical averages, to help you plan ahead.\"}]\n",
      "================================= WeatherReponse =================================\n",
      "temperature=21.3 wind_speed=16.9 wind_direction='SSW'\n"
     ]
    }
   ],
   "source": [
    "ans = app.invoke({'messages':[HumanMessage(content=\"What is the weather in Barcelona?\")]})\n",
    "for message in ans['messages']:\n",
    "    try:\n",
    "        message.pretty_print()\n",
    "    except:\n",
    "        print(\"================================= WeatherReponse =================================\")\n",
    "        print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic! Our graph returned data in the expected format, and with what looks like the correct values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's turn to using tools to make decisions in our graph, such as what node to proceed to next. To do this, we can define a pydantic model that will inform our node selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoutingFunction(BaseModel):\n",
    "    \"\"\"Select the next node to proceed to.\"\"\"\n",
    "\n",
    "    next_node: str = Field(description=\"The next node to travel to. The options are: node_2 or node_3.\")\n",
    "\n",
    "tool_node = ToolNode([RoutingFunction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's bind this \"function\" to our model and force it to select the tool, by passing in the param `tool_choice`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, streaming=True)\n",
    "model = model.bind_tools([RoutingFunction],tool_choice=\"RoutingFunction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define our new graph, first specifying the functions we will need for each of our nodes. In this example we are just testing whether our routing tool works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "def node_2_func(state):\n",
    "    return {\"messages\":[SystemMessage(content=f\"I made it to node 2!\")]}\n",
    "\n",
    "def node_3_func(state):\n",
    "    return {\"messages\":[SystemMessage(content=f\"I made it to node 3!\")]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our node functions and our graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_which_node(state: AgentState):\n",
    "    return state['messages'][-1].content.split('\\'')[1]\n",
    "\n",
    "def call_model(state: AgentState) -> AgentState:\n",
    "    return {\"messages\":[model.invoke(state['messages'])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(AgentState)\n",
    "graph.add_node(\"agent\", call_model)\n",
    "graph.add_node(\"routing_tool\", tool_node)\n",
    "graph.add_edge(\"agent\",\"routing_tool\")\n",
    "graph.add_node(\"node_2\", node_2_func)\n",
    "graph.add_node(\"node_3\", node_3_func)\n",
    "graph.add_conditional_edges(\"routing_tool\",select_which_node)\n",
    "graph.add_edge(\"node_2\",END)\n",
    "graph.add_edge(\"node_3\",END)\n",
    "graph.add_edge(START, \"agent\")\n",
    "app = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Please go to the node with the higher number'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_4tvqTghlNlCeBpyvxYj51Af2', 'function': {'arguments': '{\"next_node\":\"node_3\"}', 'name': 'RoutingFunction'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'stop'}, id='run-829f4fc0-4490-4d59-80b0-ac9c6332fd6c-0', tool_calls=[{'name': 'RoutingFunction', 'args': {'next_node': 'node_3'}, 'id': 'call_4tvqTghlNlCeBpyvxYj51Af2'}]),\n",
       "  ToolMessage(content=\"next_node='node_3'\", name='RoutingFunction', tool_call_id='call_4tvqTghlNlCeBpyvxYj51Af2'),\n",
       "  SystemMessage(content='I made it to node 3!')]}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke({\"messages\":[HumanMessage(content=\"Please go to the node with the higher number\")]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, by using structured output we can select the next node to travel to without having to explicitly mention it's name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More to explore!\n",
    "\n",
    "Hopefully this example doc gave you a good idea of a few of the ways to utilize tools and structured output in your Langgraph application. This list is neither exhaustive or mutually exclusive and there are many more exciting ways you can customize how you use tools and structured output, such as defining tools to interact with your own data, using structured output to generate content, etc. We look forward to seeing all the creative ways developers like you utilize this exciting Langgraph functionality!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
